{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "import unicodedata\n",
    "import re\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "from pytorch_lightning.metrics import Accuracy\n",
    "import torch.nn.functional as F\n",
    "import pytorch_lightning as pl\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "%matplotlib inline\n",
    "device = torch.device('cuda')     # Default CUDA device\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "!!curl -O https://www.manythings.org/anki/fra-eng.zip\n",
    "!!unzip fra-eng.zip\n",
    "file_path = \"fra.txt\"\n",
    "\n",
    "# To change to spanish, just change \"fra\" to \"spa\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn a Unicode string to plain ASCII, thanks to\n",
    "# https://stackoverflow.com/a/518232/2809427\n",
    "def unicodeToAscii(s):\n",
    "    return ''.join(c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn')\n",
    "\n",
    "# Lowercase, trim, and remove non-letter characters\n",
    "def normalizeString(s):\n",
    "    s = unicodeToAscii(s.lower().strip())\n",
    "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
    "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "PAD_token = 0 \n",
    "SOS_token = 1\n",
    "EOS_token = 2\n",
    "\n",
    "class Lang:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {PAD_token: \"PAD\", SOS_token: \"SOS\", EOS_token: \"EOS\"}\n",
    "        self.n_words = 3  # Count SOS, EOS, PAD\n",
    "    def addSentence(self, sentence):\n",
    "        # we could have also used tokenizer from https://huggingface.co/ or spaCy\n",
    "        for word in sentence.split(' '):\n",
    "            self.addWord(word)\n",
    "\n",
    "    def addWord(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.n_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.n_words] = word\n",
    "            self.n_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readLangs(file_path,lang1,lang2,reverse=False):\n",
    "    print(\"Reading lines...\")\n",
    "\n",
    "    # Read the file and split into lines\n",
    "    lines = open(file_path, encoding='utf-8').\\\n",
    "        read().strip().split('\\n')\n",
    "\n",
    "    # Split every line into pairs and normalize\n",
    "    pairs = [[normalizeString(s) for s in l.split('\\t')] for l in lines]\n",
    "    print(\"here we go\",pairs[0])\n",
    "    # Reverse pairs, make Lang instances\n",
    "    if reverse:\n",
    "        pairs = [list(reversed(p)) for p in pairs]\n",
    "        input_lang = Lang(lang2)\n",
    "        output_lang = Lang(lang1)\n",
    "    else:\n",
    "        input_lang = Lang(lang1)\n",
    "        output_lang = Lang(lang2)\n",
    "\n",
    "    return input_lang, output_lang, pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = 20\n",
    "\n",
    "def filterPair(p):\n",
    "    return len(p[0].split(' ')) < MAX_LENGTH and \\\n",
    "        len(p[1].split(' ')) < MAX_LENGTH    \n",
    "\n",
    "def filterPairs(pairs):\n",
    "    return [pair for pair in pairs if filterPair(pair)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading lines...\n",
      "here we go ['go .', 'va !']\n",
      "Read 135842 sentence pairs\n",
      "Trimmed to 135072 sentence pairs\n",
      "Counting words...\n",
      "Counted words:\n",
      "en 12892\n",
      "fr 21144\n",
      "['i hope we ll see each other again sometime .'\n",
      " 'j espere que nous nous reverrons un jour .']\n"
     ]
    }
   ],
   "source": [
    "def prepareData(file_path,lang1, lang2, reverse=False):\n",
    "    input_lang, output_lang, pairs = readLangs(file_path,lang1, lang2, reverse)\n",
    "    print(\"Read %s sentence pairs\" % len(pairs))\n",
    "    pairs = filterPairs(pairs)\n",
    "    print(\"Trimmed to %s sentence pairs\" % len(pairs))\n",
    "    print(\"Counting words...\")\n",
    "    for pair in pairs:\n",
    "        input_lang.addSentence(pair[0]) #which is english\n",
    "        output_lang.addSentence(pair[1])\n",
    "    print(\"Counted words:\")\n",
    "    print(input_lang.name, input_lang.n_words)\n",
    "    print(output_lang.name, output_lang.n_words)\n",
    "    pairs = np.asarray(pairs)\n",
    "    pairs = pairs[:,:2] #ignore the attribution part\n",
    "    return input_lang, output_lang, pairs\n",
    "\n",
    "input_lang, output_lang, pairs = prepareData(file_path,'en','fr' ,False)\n",
    "\n",
    "\n",
    "print(random.choice(pairs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[290],\n",
      "        [  2]], device='cuda:0')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array(['we won .', 'nous gagnames .'], dtype='<U138'),\n",
       " (tensor([[40],\n",
       "          [16],\n",
       "          [ 4],\n",
       "          [ 2]], device='cuda:0'),\n",
       "  tensor([[83],\n",
       "          [85],\n",
       "          [15],\n",
       "          [ 2]], device='cuda:0')))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tensorFromSentence(lang_obj, sentence):\n",
    "    sentence_indexes = [lang_obj.word2index[word] for word in sentence.split(' ')]\n",
    "    sentence_indexes.append(EOS_token)\n",
    "    return torch.tensor(sentence_indexes, dtype=torch.long).to(device).view(-1, 1)\n",
    "\n",
    "def tensorsFromPair(pair):\n",
    "    input_tensor = tensorFromSentence(input_lang, pair[0])\n",
    "    target_tensor = tensorFromSentence(output_lang, pair[1])\n",
    "    return (input_tensor, target_tensor)\n",
    "\n",
    "print(tensorFromSentence(input_lang,\"hello\"))\n",
    "pairs[55],tensorsFromPair(pairs[55])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bahdanau Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BahdanauAttnDecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, n_layers=1, dropout_p=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Define parameters\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.dropout_p = dropout_p\n",
    "\n",
    "        # Define layers\n",
    "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "        self.attn = nn.Linear(self.hidden_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size * 2, hidden_size, n_layers, dropout=dropout_p)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, word_input, last_hidden, encoder_outputs):\n",
    "        # Note that we will only be running forward for a single decoder time step, but will use all encoder outputs\n",
    "        \n",
    "        # Get the embedding of the current input word (last output word)\n",
    "        word_embedded = self.embedding(word_input).view(1, 1, -1) # S=1 x B x N\n",
    "        word_embedded = self.dropout(word_embedded)\n",
    "        \n",
    "        # Calculate attention weights and apply to encoder outputs\n",
    "        attn_weights = self.attn(last_hidden[-1], encoder_outputs)\n",
    "        context = attn_weights.bmm(encoder_outputs.transpose(0, 1)) # B x 1 x N\n",
    "        \n",
    "        # Combine embedded input word and attended context, run through RNN\n",
    "        rnn_input = torch.cat((word_embedded, context), 2)\n",
    "        output, hidden = self.gru(rnn_input, last_hidden)\n",
    "        \n",
    "        # Final output layer\n",
    "        output = output.squeeze(0) # B x N\n",
    "        output = F.log_softmax(self.out(torch.cat((output, context), 1)))\n",
    "        \n",
    "        # Return final output, hidden state, and attention weights (for visualization)\n",
    "        return output, hidden, attn_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoder Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRNN(pl.LightningModule):\n",
    "    def __init__(self, input_size, embedding_dim, n_units, n_layers=1):\n",
    "        super().__init__()\n",
    "        self.n_units = n_units\n",
    "        self.n_layers = n_layers\n",
    "        self.embedding = nn.Embedding(num_embeddings=input_size,embedding_dim=embedding_dim)\n",
    "        self.gru = nn.GRU(input_size = embedding_dim, hidden_size = n_units, num_layers=n_layers,batch_first =True)\n",
    "\n",
    "    def forward(self, input, hidden,seq_length):\n",
    "        embedded = self.embedding(input.to(device)).squeeze(-2).to(device)\n",
    "        #embedded If batch_first is True, B x T x * input is expected.\n",
    "        packed = torch.nn.utils.rnn.pack_padded_sequence(embedded, seq_length.cpu(),batch_first=True,enforce_sorted=True)\n",
    "\n",
    "        outputs, hidden = self.gru(packed, hidden)\n",
    "        \n",
    "        outputs, _ = torch.nn.utils.rnn.pad_packed_sequence(outputs,batch_first=True)\n",
    "        return outputs, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(self.n_layers, 1, self.n_units).to(device) #(num_layers * num_directions, batch, hidden_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Luong Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttnLayer(pl.LightningModule):\n",
    "    def __init__(self, method, hidden_size):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.method = method\n",
    "        if self.method not in ['dot', 'general', 'concat']:\n",
    "            raise ValueError(self.method, \"is not an appropriate attention method.\")\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        self.attn_general = nn.Linear(self.hidden_size, self.hidden_size)\n",
    "\n",
    "        self.attn_concat = nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
    "        self.v = nn.Parameter(torch.FloatTensor(1, self.hidden_size))\n",
    "\n",
    "    def forward(self, hidden, encoder_outputs):\n",
    "        seq_len = len(encoder_outputs)\n",
    "\n",
    "        # Create variable to store attention energies\n",
    "        attn_energies = torch.zeros(seq_len).to(device) # B x 1 x S\n",
    "\n",
    "        # Calculate energies for each encoder output\n",
    "        for i in range(seq_len):\n",
    "            attn_energies[i] = self.score(hidden, encoder_outputs[i])\n",
    "\n",
    "        # Normalize energies to weights in range 0 to 1, resize to 1 x 1 x seq_len\n",
    "        return F.softmax(attn_energies,0).unsqueeze(0).unsqueeze(0) #it was oritnally ([20]) but we return it as (1,,1,20)\n",
    "    \n",
    " \n",
    "    \n",
    "    def score(self, hidden, encoder_outputs):\n",
    "        hidden = hidden.squeeze(0)\n",
    "        encoder_outputs = encoder_outputs.squeeze(0)\n",
    "        \n",
    "        if self.method == 'general':\n",
    "            attn_energies = self.attn_general(encoder_outputs)\n",
    "            attn_energies = hidden.dot(attn_energies)\n",
    "        elif self.method == 'concat':\n",
    "            attn_energies = torch.tanh(self.attn_concat(torch.cat((hidden, encoder_outputs), 1)))\n",
    "            attn_energies = self.v.dot(attn_energies)\n",
    "        else:\n",
    "        #elif self.method == 'dot':\n",
    "            attn_energies = hidden.dot(encoder_outputs)\n",
    "\n",
    "        return attn_energies\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LuongAttnDecoderRNN(pl.LightningModule):\n",
    "    def __init__(self, attn_model, hidden_size, output_size, n_layers=1, dropout_p=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Keep parameters for reference\n",
    "        self.attn_model = attn_model\n",
    "        self.hidden_size = hidden_size #(num_layers * num_directions, batch, hidden_size), so we should get same hidden size from the prev one\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.dropout_p = dropout_p\n",
    "        # dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
    "        # Define layers\n",
    "        self.embedding = nn.Embedding(self.output_size, self.hidden_size)\n",
    "        self.gru = nn.GRU(self.hidden_size * 2, self.hidden_size, n_layers, dropout=dropout_p)\n",
    "        self.out = nn.Linear(self.hidden_size * 2, self.output_size)\n",
    "        \n",
    "        # Choose attention model\n",
    "        if attn_model != 'none':\n",
    "            self.attn = AttnLayer(attn_model, hidden_size)\n",
    "    \n",
    "    def forward(self, input, last_context, last_hidden, encoder_outputs):\n",
    "        # Note: we run this one step at a time\n",
    "        \n",
    "        # Get the embedding of the current input word (last output word)\n",
    "        embedded = self.embedding(input).view(1, 1, -1) # S= 1 x 1 x N\n",
    "        \n",
    "        # Combine embedded input word and last context, run through RNN\n",
    "        rnn_input = torch.cat((embedded, last_context.unsqueeze(0)), 2)\n",
    "        rnn_output, hidden = self.gru(rnn_input, last_hidden)\n",
    "        \n",
    "        # Calculate attention from current Decoder RNN state and all encoder hidden states; \n",
    "        attn_weights = self.attn(rnn_output.squeeze(0), encoder_outputs)\n",
    "        \n",
    "        #to get the context vector it should be the size of the n.units so we can feed it with the rnn_output\n",
    "        #bmm needs 3D tensor as the first one must be the batch\n",
    "        context = attn_weights.bmm(encoder_outputs.unsqueeze(0)) # B x 1 x N\n",
    "        # Final output layer (next word prediction) using the RNN hidden state and context vector\n",
    "        rnn_output = rnn_output.squeeze(0) # S=1 x B x N -> B x N\n",
    "        \n",
    "        context = context.squeeze(1)       # B x S=1 x N -> B x N\n",
    "        output = F.log_softmax(self.out(torch.cat((rnn_output, context), 1)),1) #([1, 23334]) for one word only we normalize\n",
    "        \n",
    "        # Return final output, hidden state, and attention weights (for visualization)\n",
    "        return output, context, hidden, attn_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing Attentions in Decoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class Seq2seqModel(pl.LightningModule):\n",
    "    def __init__(self,enc,dec,index2word,n_vocab_target,max_length=MAX_LENGTH):\n",
    "        super().__init__()\n",
    "        # need to be (self) in order to be optimized and part of model\n",
    "        self.encoder = enc\n",
    "        self.decoder = dec\n",
    "        self.max_length = max_length\n",
    "        self.n_vocab_target=n_vocab_target\n",
    "        self.teacher_forcing_ratio = 0.5\n",
    "        self.criterion = nn.NLLLoss()\n",
    "        self.acc = Accuracy()\n",
    "        self.index2word = index2word\n",
    "        self.PAD_token = 0 \n",
    "        self.SOS_token = 1\n",
    "        self.EOS_token = 2\n",
    "    def forward(self,input_tensor,target_tensor,seq_length):\n",
    "        encoder_hidden = self.encoder.initHidden()\n",
    "        encoder_output, encoder_hidden = self.encoder(input_tensor, encoder_hidden,seq_length) # we pass the seq_length to pack and unpack\n",
    "        target_length = target_tensor.size(1)  #The max number of words for the target lang\n",
    "        # now the encoder_output is [1, 4, 64], as each word has n_units, we want the output to be [20,64]\n",
    "        encoder_outputs = torch.zeros(self.max_length, self.encoder.n_units).to(device)\n",
    "        for i in range(encoder_output.size(1)):\n",
    "            encoder_outputs[i] = encoder_output[0,i]\n",
    "        loss=0\n",
    "        acc =0\n",
    "        decoder_input = torch.tensor([[self.SOS_token]]).to(device)# SOS_TOKEN but we pass a value (1) to be a local variable not global in torch script\n",
    "\n",
    "        decoder_hidden = encoder_hidden\n",
    "\n",
    "        use_teacher_forcing = torch.rand(1) < self.teacher_forcing_ratio\n",
    "        decoder_context = torch.zeros(1, self.decoder.hidden_size).to(device)\n",
    "\n",
    "        decoder_outputs = torch.zeros(target_length, self.n_vocab_target).to(device)\n",
    "\n",
    "        if use_teacher_forcing:\n",
    "            # Teacher forcing: Feed the target as the next input\n",
    "            for di in range(target_length):\n",
    "                decoder_output, decoder_context, decoder_hidden, decoder_attn = self.decoder(decoder_input,decoder_context, decoder_hidden, encoder_outputs)\n",
    "                # loss = nn.NLLLoss(decoder_output, target_tensor[0,di])\n",
    "                decoder_input = target_tensor[0,di]  # Teacher forcing\n",
    "                decoder_outputs[di] = decoder_output\n",
    "                if decoder_input.item() in [self.EOS_token ,self.PAD_token]:\n",
    "                    break\n",
    "\n",
    "            # The decoder outputs has the shape (N,C) which is (4 words, 23334 features)\n",
    "            # target_tensor.squeeze() will be only (N)\n",
    "            loss = self.criterion(decoder_outputs, target_tensor.squeeze())\n",
    "            acc=self.acc(torch.argmax(decoder_outputs, dim=1).to(device),target_tensor.squeeze())\n",
    "            \n",
    "\n",
    "        else:\n",
    "            # Without teacher forcing: use its own predictions as the next input\n",
    "            for di in range(target_length):\n",
    "                decoder_output, decoder_context, decoder_hidden, decoder_attn = self.decoder(decoder_input,decoder_context, decoder_hidden, encoder_outputs)\n",
    "                topv, topi = decoder_output.topk(1)\n",
    "                decoder_input = topi.squeeze().detach()  # detach from history as input\n",
    "                decoder_outputs[di] = decoder_output\n",
    "                if decoder_input.item() in [self.EOS_token ,self.PAD_token]:\n",
    "                    break\n",
    "            loss = self.criterion(decoder_outputs, target_tensor.squeeze())\n",
    "            acc=self.acc(torch.argmax(decoder_outputs, dim=1),target_tensor.squeeze())\n",
    "            \n",
    "\n",
    "        return loss,acc,decoder_outputs\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        input_tensor, target_tensor,seq_length = batch\n",
    "        loss,acc,y_hat =self(input_tensor, target_tensor,seq_length) #y_hat is size (N,C)\n",
    "        self.log('loss',loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "        self.log('accuracy',acc, on_step=False, on_epoch=True, prog_bar=True, logger=True)\n",
    "#         self.logger.experiment.add_graph(self, (input_tensor, target_tensor,seq_length ))\n",
    "        \n",
    "        return {'loss': loss,'accuracy':acc}\n",
    "    \n",
    "    def translate(self,sentence):\n",
    "        sentence = normalizeString(sentence)\n",
    "        input_tensor = tensorFromSentence(input_lang, sentence).unsqueeze(0).to(device)\n",
    "        seq_lengths = torch.LongTensor(list(map(len, input_tensor))).to(device)\n",
    "        self.encoder_hidden = self.encoder.initHidden()\n",
    "        encoder_outputs = torch.zeros(self.max_length, self.encoder.n_units).to(device)\n",
    "\n",
    "        encoder_output, encoder_hidden = self.encoder(input_tensor, self.encoder_hidden,seq_lengths) # we pass the seq_length to pack and unpack which is how many words\n",
    "        \n",
    "        for i in range(encoder_output.size(1)):\n",
    "            encoder_outputs[i] = encoder_output[0,i]\n",
    "        decoder_input = torch.tensor([[self.SOS_token]]).to(device)# SOS\n",
    "\n",
    "        decoder_hidden = encoder_hidden\n",
    "\n",
    "        decoded_words = []\n",
    "        decoder_context = torch.zeros(1, self.decoder.hidden_size).to(device)\n",
    "        decoder_attentions = torch.zeros(self.max_length, self.max_length).to(device)\n",
    "\n",
    "        for di in range(self.max_length):\n",
    "            decoder_output, decoder_context, decoder_hidden, decoder_attn = self.decoder(decoder_input,decoder_context, decoder_hidden, encoder_outputs)\n",
    "            decoder_attentions[di] = decoder_attn.data\n",
    "            topv, topi = decoder_output.data.topk(1)\n",
    "            if topi.item() == self.EOS_token:\n",
    "                decoded_words.append('<EOS>')\n",
    "                break\n",
    "            else:\n",
    "                decoded_words.append(self.index2word[topi.item()])\n",
    "\n",
    "            decoder_input = topi.squeeze().detach()\n",
    "        \n",
    "        decoded_words=' '.join(decoded_words)\n",
    "        return decoded_words, decoder_attentions[:di + 1]\n",
    " \n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: None, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "   | Name                      | Type                | Params\n",
      "-------------------------------------------------------------------\n",
      "0  | encoder                   | EncoderRNN          | 3.4 M \n",
      "1  | encoder.embedding         | Embedding           | 3.3 M \n",
      "2  | encoder.gru               | GRU                 | 61.8 K\n",
      "3  | decoder                   | LuongAttnDecoderRNN | 4.1 M \n",
      "4  | decoder.embedding         | Embedding           | 1.4 M \n",
      "5  | decoder.gru               | GRU                 | 37.2 K\n",
      "6  | decoder.out               | Linear              | 2.7 M \n",
      "7  | decoder.attn              | AttnLayer           | 12.5 K\n",
      "8  | decoder.attn.attn_general | Linear              | 4.2 K \n",
      "9  | decoder.attn.attn_concat  | Linear              | 8.3 K \n",
      "10 | criterion                 | NLLLoss             | 0     \n",
      "11 | acc                       | Accuracy            | 0     \n",
      "-------------------------------------------------------------------\n",
      "7.5 M     Trainable params\n",
      "0         Non-trainable params\n",
      "7.5 M     Total params\n",
      "29.971    Total estimated model params size (MB)\n",
      "/rhome/yelnady/.local/lib/python3.6/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 64 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a8276a709244122b5491ab5b7239913",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Padding->DataLoader->Trainer->fit->test\n",
    "hidden_size = 64\n",
    "embedding_dim = 256\n",
    "params = {'shuffle': True,'num_workers': 0,'batch_size':1}\n",
    "device = torch.device('cuda')     # Default CUDA device\n",
    "input_tensor = [tensorsFromPair(pair)[0] for pair in pairs[:5]]\n",
    "# seq_lengths: Gets the actual sequence length of each timestep, this one is from YouTUbe \n",
    "seq_lengths = torch.LongTensor(list(map(len, input_tensor))).to(device)\n",
    "# input_tensor is [100,4,1] 100 word, each is 4 words, each is one feature (which is the index number)\n",
    "input_tensor = torch.nn.utils.rnn.pad_sequence(input_tensor, batch_first=True).to(device)\n",
    "target_tensor = [tensorsFromPair(pair)[1] for pair in pairs[:5]]\n",
    "target_tensor = torch.nn.utils.rnn.pad_sequence(target_tensor, batch_first=True).to(device)\n",
    "\n",
    "\n",
    "dataset = torch.utils.data.TensorDataset(input_tensor,target_tensor,seq_lengths) # You msut set everything to be in Cuda\n",
    "train_dataloader  = torch.utils.data.DataLoader(dataset, **params)\n",
    "\n",
    "print(target_tensor.device)\n",
    "encoder = EncoderRNN(input_lang.n_words,embedding_dim, hidden_size,n_layers=1).to(device)\n",
    "attn_decoder = LuongAttnDecoderRNN('general',hidden_size, output_lang.n_words, n_layers=1).to(device)\n",
    "model = Seq2seqModel(encoder,attn_decoder,output_lang.index2word,output_lang.n_words).to(device)\n",
    "\n",
    "trainer = pl.Trainer(gpus=1,max_epochs=2,progress_bar_refresh_rate=1,weights_summary='full')\n",
    "trainer.fit(model,train_dataloader)\n",
    "\n",
    "\n",
    "\n",
    "# By checking print((\"from the model\",model.encoder.embedding.weight)) and print((\"out of the model\",encoder1.embedding.weight))\n",
    "# We know that passed encoder is trained and the reference is the passed not a new copy\n",
    "\n",
    "# How to save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f96ee7ca518>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x144 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "device = torch.device('cpu')     # Default to Cpu after you have finished training\n",
    "\n",
    "# model.evaluate()\n",
    "sentence ,attentions = model.translate('run')\n",
    "PATH = \"entire_model.pt\"\n",
    "torch.save(model, PATH)\n",
    "# model = torch.load(PATH)\n",
    "plt.matshow(attentions.numpy())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=======================================================================================\n",
      "                                     Kernel Shape    Output Shape     Params  \\\n",
      "Layer                                                                          \n",
      "0_encoder.Embedding_embedding        [256, 14241]  [1, 3, 1, 256]  3.645696M   \n",
      "1_encoder.GRU_gru                               -         [1, 64]    61.824k   \n",
      "2_decoder.Embedding_embedding         [64, 23334]      [1, 1, 64]  1.493376M   \n",
      "3_decoder.GRU_gru                               -      [1, 1, 64]    37.248k   \n",
      "4_decoder.attn.Linear_attn_general       [64, 64]            [64]      4.16k   \n",
      "5_decoder.attn.Linear_attn_general       [64, 64]            [64]          -   \n",
      "6_decoder.attn.Linear_attn_general       [64, 64]            [64]          -   \n",
      "7_decoder.attn.Linear_attn_general       [64, 64]            [64]          -   \n",
      "8_decoder.attn.Linear_attn_general       [64, 64]            [64]          -   \n",
      "9_decoder.attn.Linear_attn_general       [64, 64]            [64]          -   \n",
      "10_decoder.attn.Linear_attn_general      [64, 64]            [64]          -   \n",
      "11_decoder.attn.Linear_attn_general      [64, 64]            [64]          -   \n",
      "12_decoder.attn.Linear_attn_general      [64, 64]            [64]          -   \n",
      "13_decoder.attn.Linear_attn_general      [64, 64]            [64]          -   \n",
      "14_decoder.attn.Linear_attn_general      [64, 64]            [64]          -   \n",
      "15_decoder.attn.Linear_attn_general      [64, 64]            [64]          -   \n",
      "16_decoder.attn.Linear_attn_general      [64, 64]            [64]          -   \n",
      "17_decoder.attn.Linear_attn_general      [64, 64]            [64]          -   \n",
      "18_decoder.attn.Linear_attn_general      [64, 64]            [64]          -   \n",
      "19_decoder.attn.Linear_attn_general      [64, 64]            [64]          -   \n",
      "20_decoder.attn.Linear_attn_general      [64, 64]            [64]          -   \n",
      "21_decoder.attn.Linear_attn_general      [64, 64]            [64]          -   \n",
      "22_decoder.attn.Linear_attn_general      [64, 64]            [64]          -   \n",
      "23_decoder.attn.Linear_attn_general      [64, 64]            [64]          -   \n",
      "24_decoder.Linear_out                [128, 23334]      [1, 23334]  3.010086M   \n",
      "25_decoder.Embedding_embedding        [64, 23334]            [64]          -   \n",
      "26_decoder.GRU_gru                              -      [1, 1, 64]          -   \n",
      "27_decoder.attn.Linear_attn_general      [64, 64]            [64]          -   \n",
      "28_decoder.attn.Linear_attn_general      [64, 64]            [64]          -   \n",
      "29_decoder.attn.Linear_attn_general      [64, 64]            [64]          -   \n",
      "30_decoder.attn.Linear_attn_general      [64, 64]            [64]          -   \n",
      "31_decoder.attn.Linear_attn_general      [64, 64]            [64]          -   \n",
      "32_decoder.attn.Linear_attn_general      [64, 64]            [64]          -   \n",
      "33_decoder.attn.Linear_attn_general      [64, 64]            [64]          -   \n",
      "34_decoder.attn.Linear_attn_general      [64, 64]            [64]          -   \n",
      "35_decoder.attn.Linear_attn_general      [64, 64]            [64]          -   \n",
      "36_decoder.attn.Linear_attn_general      [64, 64]            [64]          -   \n",
      "37_decoder.attn.Linear_attn_general      [64, 64]            [64]          -   \n",
      "38_decoder.attn.Linear_attn_general      [64, 64]            [64]          -   \n",
      "39_decoder.attn.Linear_attn_general      [64, 64]            [64]          -   \n",
      "40_decoder.attn.Linear_attn_general      [64, 64]            [64]          -   \n",
      "41_decoder.attn.Linear_attn_general      [64, 64]            [64]          -   \n",
      "42_decoder.attn.Linear_attn_general      [64, 64]            [64]          -   \n",
      "43_decoder.attn.Linear_attn_general      [64, 64]            [64]          -   \n",
      "44_decoder.attn.Linear_attn_general      [64, 64]            [64]          -   \n",
      "45_decoder.attn.Linear_attn_general      [64, 64]            [64]          -   \n",
      "46_decoder.attn.Linear_attn_general      [64, 64]            [64]          -   \n",
      "47_decoder.Linear_out                [128, 23334]      [1, 23334]          -   \n",
      "48_decoder.Embedding_embedding        [64, 23334]            [64]          -   \n",
      "49_decoder.GRU_gru                              -      [1, 1, 64]          -   \n",
      "50_decoder.attn.Linear_attn_general      [64, 64]            [64]          -   \n",
      "51_decoder.attn.Linear_attn_general      [64, 64]            [64]          -   \n",
      "52_decoder.attn.Linear_attn_general      [64, 64]            [64]          -   \n",
      "53_decoder.attn.Linear_attn_general      [64, 64]            [64]          -   \n",
      "54_decoder.attn.Linear_attn_general      [64, 64]            [64]          -   \n",
      "55_decoder.attn.Linear_attn_general      [64, 64]            [64]          -   \n",
      "56_decoder.attn.Linear_attn_general      [64, 64]            [64]          -   \n",
      "57_decoder.attn.Linear_attn_general      [64, 64]            [64]          -   \n",
      "58_decoder.attn.Linear_attn_general      [64, 64]            [64]          -   \n",
      "59_decoder.attn.Linear_attn_general      [64, 64]            [64]          -   \n",
      "60_decoder.attn.Linear_attn_general      [64, 64]            [64]          -   \n",
      "61_decoder.attn.Linear_attn_general      [64, 64]            [64]          -   \n",
      "62_decoder.attn.Linear_attn_general      [64, 64]            [64]          -   \n",
      "63_decoder.attn.Linear_attn_general      [64, 64]            [64]          -   \n",
      "64_decoder.attn.Linear_attn_general      [64, 64]            [64]          -   \n",
      "65_decoder.attn.Linear_attn_general      [64, 64]            [64]          -   \n",
      "66_decoder.attn.Linear_attn_general      [64, 64]            [64]          -   \n",
      "67_decoder.attn.Linear_attn_general      [64, 64]            [64]          -   \n",
      "68_decoder.attn.Linear_attn_general      [64, 64]            [64]          -   \n",
      "69_decoder.attn.Linear_attn_general      [64, 64]            [64]          -   \n",
      "70_decoder.Linear_out                [128, 23334]      [1, 23334]          -   \n",
      "71_criterion                                    -              []          -   \n",
      "72_acc                                          -              []          -   \n",
      "\n",
      "                                     Mult-Adds  \n",
      "Layer                                           \n",
      "0_encoder.Embedding_embedding        3.645696M  \n",
      "1_encoder.GRU_gru                       61.44k  \n",
      "2_decoder.Embedding_embedding        1.493376M  \n",
      "3_decoder.GRU_gru                      36.864k  \n",
      "4_decoder.attn.Linear_attn_general      4.096k  \n",
      "5_decoder.attn.Linear_attn_general      4.096k  \n",
      "6_decoder.attn.Linear_attn_general      4.096k  \n",
      "7_decoder.attn.Linear_attn_general      4.096k  \n",
      "8_decoder.attn.Linear_attn_general      4.096k  \n",
      "9_decoder.attn.Linear_attn_general      4.096k  \n",
      "10_decoder.attn.Linear_attn_general     4.096k  \n",
      "11_decoder.attn.Linear_attn_general     4.096k  \n",
      "12_decoder.attn.Linear_attn_general     4.096k  \n",
      "13_decoder.attn.Linear_attn_general     4.096k  \n",
      "14_decoder.attn.Linear_attn_general     4.096k  \n",
      "15_decoder.attn.Linear_attn_general     4.096k  \n",
      "16_decoder.attn.Linear_attn_general     4.096k  \n",
      "17_decoder.attn.Linear_attn_general     4.096k  \n",
      "18_decoder.attn.Linear_attn_general     4.096k  \n",
      "19_decoder.attn.Linear_attn_general     4.096k  \n",
      "20_decoder.attn.Linear_attn_general     4.096k  \n",
      "21_decoder.attn.Linear_attn_general     4.096k  \n",
      "22_decoder.attn.Linear_attn_general     4.096k  \n",
      "23_decoder.attn.Linear_attn_general     4.096k  \n",
      "24_decoder.Linear_out                2.986752M  \n",
      "25_decoder.Embedding_embedding       1.493376M  \n",
      "26_decoder.GRU_gru                     36.864k  \n",
      "27_decoder.attn.Linear_attn_general     4.096k  \n",
      "28_decoder.attn.Linear_attn_general     4.096k  \n",
      "29_decoder.attn.Linear_attn_general     4.096k  \n",
      "30_decoder.attn.Linear_attn_general     4.096k  \n",
      "31_decoder.attn.Linear_attn_general     4.096k  \n",
      "32_decoder.attn.Linear_attn_general     4.096k  \n",
      "33_decoder.attn.Linear_attn_general     4.096k  \n",
      "34_decoder.attn.Linear_attn_general     4.096k  \n",
      "35_decoder.attn.Linear_attn_general     4.096k  \n",
      "36_decoder.attn.Linear_attn_general     4.096k  \n",
      "37_decoder.attn.Linear_attn_general     4.096k  \n",
      "38_decoder.attn.Linear_attn_general     4.096k  \n",
      "39_decoder.attn.Linear_attn_general     4.096k  \n",
      "40_decoder.attn.Linear_attn_general     4.096k  \n",
      "41_decoder.attn.Linear_attn_general     4.096k  \n",
      "42_decoder.attn.Linear_attn_general     4.096k  \n",
      "43_decoder.attn.Linear_attn_general     4.096k  \n",
      "44_decoder.attn.Linear_attn_general     4.096k  \n",
      "45_decoder.attn.Linear_attn_general     4.096k  \n",
      "46_decoder.attn.Linear_attn_general     4.096k  \n",
      "47_decoder.Linear_out                2.986752M  \n",
      "48_decoder.Embedding_embedding       1.493376M  \n",
      "49_decoder.GRU_gru                     36.864k  \n",
      "50_decoder.attn.Linear_attn_general     4.096k  \n",
      "51_decoder.attn.Linear_attn_general     4.096k  \n",
      "52_decoder.attn.Linear_attn_general     4.096k  \n",
      "53_decoder.attn.Linear_attn_general     4.096k  \n",
      "54_decoder.attn.Linear_attn_general     4.096k  \n",
      "55_decoder.attn.Linear_attn_general     4.096k  \n",
      "56_decoder.attn.Linear_attn_general     4.096k  \n",
      "57_decoder.attn.Linear_attn_general     4.096k  \n",
      "58_decoder.attn.Linear_attn_general     4.096k  \n",
      "59_decoder.attn.Linear_attn_general     4.096k  \n",
      "60_decoder.attn.Linear_attn_general     4.096k  \n",
      "61_decoder.attn.Linear_attn_general     4.096k  \n",
      "62_decoder.attn.Linear_attn_general     4.096k  \n",
      "63_decoder.attn.Linear_attn_general     4.096k  \n",
      "64_decoder.attn.Linear_attn_general     4.096k  \n",
      "65_decoder.attn.Linear_attn_general     4.096k  \n",
      "66_decoder.attn.Linear_attn_general     4.096k  \n",
      "67_decoder.attn.Linear_attn_general     4.096k  \n",
      "68_decoder.attn.Linear_attn_general     4.096k  \n",
      "69_decoder.attn.Linear_attn_general     4.096k  \n",
      "70_decoder.Linear_out                2.986752M  \n",
      "71_criterion                                 -  \n",
      "72_acc                                       -  \n",
      "---------------------------------------------------------------------------------------\n",
      "                          Totals\n",
      "Total params            8.25239M\n",
      "Trainable params        8.25239M\n",
      "Non-trainable params         0.0\n",
      "Mult-Adds             17.503872M\n",
      "=======================================================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Kernel Shape</th>\n",
       "      <th>Output Shape</th>\n",
       "      <th>Params</th>\n",
       "      <th>Mult-Adds</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Layer</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0_encoder.Embedding_embedding</th>\n",
       "      <td>[256, 14241]</td>\n",
       "      <td>[1, 3, 1, 256]</td>\n",
       "      <td>3645696.0</td>\n",
       "      <td>3645696.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1_encoder.GRU_gru</th>\n",
       "      <td>-</td>\n",
       "      <td>[1, 64]</td>\n",
       "      <td>61824.0</td>\n",
       "      <td>61440.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2_decoder.Embedding_embedding</th>\n",
       "      <td>[64, 23334]</td>\n",
       "      <td>[1, 1, 64]</td>\n",
       "      <td>1493376.0</td>\n",
       "      <td>1493376.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3_decoder.GRU_gru</th>\n",
       "      <td>-</td>\n",
       "      <td>[1, 1, 64]</td>\n",
       "      <td>37248.0</td>\n",
       "      <td>36864.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4_decoder.attn.Linear_attn_general</th>\n",
       "      <td>[64, 64]</td>\n",
       "      <td>[64]</td>\n",
       "      <td>4160.0</td>\n",
       "      <td>4096.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68_decoder.attn.Linear_attn_general</th>\n",
       "      <td>[64, 64]</td>\n",
       "      <td>[64]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4096.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69_decoder.attn.Linear_attn_general</th>\n",
       "      <td>[64, 64]</td>\n",
       "      <td>[64]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4096.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70_decoder.Linear_out</th>\n",
       "      <td>[128, 23334]</td>\n",
       "      <td>[1, 23334]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2986752.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71_criterion</th>\n",
       "      <td>-</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72_acc</th>\n",
       "      <td>-</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>73 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     Kernel Shape    Output Shape     Params  \\\n",
       "Layer                                                                          \n",
       "0_encoder.Embedding_embedding        [256, 14241]  [1, 3, 1, 256]  3645696.0   \n",
       "1_encoder.GRU_gru                               -         [1, 64]    61824.0   \n",
       "2_decoder.Embedding_embedding         [64, 23334]      [1, 1, 64]  1493376.0   \n",
       "3_decoder.GRU_gru                               -      [1, 1, 64]    37248.0   \n",
       "4_decoder.attn.Linear_attn_general       [64, 64]            [64]     4160.0   \n",
       "...                                           ...             ...        ...   \n",
       "68_decoder.attn.Linear_attn_general      [64, 64]            [64]        NaN   \n",
       "69_decoder.attn.Linear_attn_general      [64, 64]            [64]        NaN   \n",
       "70_decoder.Linear_out                [128, 23334]      [1, 23334]        NaN   \n",
       "71_criterion                                    -              []        NaN   \n",
       "72_acc                                          -              []        NaN   \n",
       "\n",
       "                                     Mult-Adds  \n",
       "Layer                                           \n",
       "0_encoder.Embedding_embedding        3645696.0  \n",
       "1_encoder.GRU_gru                      61440.0  \n",
       "2_decoder.Embedding_embedding        1493376.0  \n",
       "3_decoder.GRU_gru                      36864.0  \n",
       "4_decoder.attn.Linear_attn_general      4096.0  \n",
       "...                                        ...  \n",
       "68_decoder.attn.Linear_attn_general     4096.0  \n",
       "69_decoder.attn.Linear_attn_general     4096.0  \n",
       "70_decoder.Linear_out                2986752.0  \n",
       "71_criterion                               NaN  \n",
       "72_acc                                     NaN  \n",
       "\n",
       "[73 rows x 4 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchsummaryX import summary\n",
    "summary(model, x1,x2,x3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchviz import make_dot\n",
    "\n",
    "\n",
    "x1 = torch.randn(1, 3, 1, requires_grad=True).long()\n",
    "x2 = torch.randn(1, 3, 1,requires_grad=True).long()\n",
    "x3 = torch.ones(1).long()\n",
    "loss,acc,y = model(x1,x2,x3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/rhome/yelnady/.local/lib/python3.6/site-packages/ipykernel_launcher.py:22: TracerWarning: Converting a tensor to a Python index might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "/rhome/yelnady/.local/lib/python3.6/site-packages/ipykernel_launcher.py:26: TracerWarning: torch.tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.\n",
      "/rhome/yelnady/.local/lib/python3.6/site-packages/ipykernel_launcher.py:35: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "/rhome/yelnady/.local/lib/python3.6/site-packages/ipykernel_launcher.py:37: TracerWarning: Converting a tensor to a Python index might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "/rhome/yelnady/.local/lib/python3.6/site-packages/ipykernel_launcher.py:17: TracerWarning: Converting a tensor to a Python index might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "/rhome/yelnady/.local/lib/python3.6/site-packages/ipykernel_launcher.py:42: TracerWarning: Converting a tensor to a Python number might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "/rhome/yelnady/.local/lib/python3.6/site-packages/pytorch_lightning/metrics/classification/helpers.py:423: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if preds.shape[0] == 1:\n",
      "/rhome/yelnady/.local/lib/python3.6/site-packages/pytorch_lightning/metrics/classification/helpers.py:63: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if target.min() < 0:\n",
      "/rhome/yelnady/.local/lib/python3.6/site-packages/pytorch_lightning/metrics/classification/helpers.py:67: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if not preds_float and preds.min() < 0:\n",
      "/rhome/yelnady/.local/lib/python3.6/site-packages/pytorch_lightning/metrics/classification/helpers.py:70: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if not preds.shape[0] == target.shape[0]:\n",
      "/rhome/yelnady/.local/lib/python3.6/site-packages/pytorch_lightning/metrics/classification/helpers.py:102: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if preds.shape != target.shape:\n",
      "/rhome/yelnady/.local/lib/python3.6/site-packages/pytorch_lightning/metrics/classification/helpers.py:307: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if preds.shape != target.shape:\n",
      "/rhome/yelnady/.local/lib/python3.6/site-packages/pytorch_lightning/metrics/classification/helpers.py:454: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  num_classes = num_classes if num_classes else max(preds.max(), target.max()) + 1\n",
      "/rhome/yelnady/.local/lib/python3.6/site-packages/pytorch_lightning/metrics/classification/helpers.py:455: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  preds = to_onehot(preds, max(2, num_classes))\n",
      "/rhome/yelnady/.local/lib/python3.6/site-packages/pytorch_lightning/metrics/classification/helpers.py:457: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  target = to_onehot(target, max(2, num_classes))\n",
      "/rhome/yelnady/.local/lib/python3.6/site-packages/torch/jit/_trace.py:966: TracerWarning: Output nr 1. of the traced function does not match the corresponding output of the Python function. Detailed error:\n",
      "With rtol=1e-05 and atol=1e-05, found 1 element(s) (out of 1) whose difference(s) exceeded the margin of error (including 0 nan comparisons). The greatest difference was 6.564022779464722 (3.290539026260376 vs. 9.854561805725098), which occurred at index 0.\n",
      "  _module_class,\n",
      "/rhome/yelnady/.local/lib/python3.6/site-packages/torch/jit/_trace.py:966: TracerWarning: Output nr 3. of the traced function does not match the corresponding output of the Python function. Detailed error:\n",
      "With rtol=1e-05 and atol=1e-05, found 42288 element(s) (out of 63432) whose difference(s) exceeded the margin of error (including 0 nan comparisons). The greatest difference was 10.500825881958008 (0.0 vs. -10.500825881958008), which occurred at index (2, 20970).\n",
      "  _module_class,\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "writer = SummaryWriter('runs/attention_model')\n",
    "writer.add_graph(model,[x1,x2,x3])\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-02-20 17:33:40.505518: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
      "TensorBoard 2.4.1 at http://localhost:6005/ (Press CTRL+C to quit)\n",
      "^C\n"
     ]
    }
   ],
   "source": [
    "!tensorboard --logdir=runs --host=localhost --port=6005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#requires having graphviz in your path\n",
    "make_dot(y,params=dict(model.named_parameters())).render(\"make_dot\", format=\"png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.lib.display import Audio\n",
    "def beep():\n",
    "    return Audio('bbc.mp3', autoplay=True)\n",
    "beep()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def speak(text):\n",
    "    from IPython.display import Javascript as js, clear_output\n",
    "    # Escape single quotes\n",
    "    text = text.replace(\"'\", r\"\\'\")\n",
    "    display(js(f'''\n",
    "    if(window.speechSynthesis) {{\n",
    "        var synth = window.speechSynthesis;\n",
    "        synth.speak(new window.SpeechSynthesisUtterance('{text}'));\n",
    "    }}\n",
    "    '''))\n",
    "    clear_output(False)\n",
    "    \n",
    "speak(\"Code Executed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TorchScript allows you to serialize your models in a way that it can be loaded in non-Python environments. \n",
    "# The LightningModule has a handy method to_torchscript() that returns a scripted module which you can save or directly use.\n",
    "\n",
    "\n",
    "script = model.to_torchscript()\n",
    "# torch.onnx.export(model, (input_tensor,target_tensor,seq_lengths), \"alexnet.onnx\", verbose=True)\n",
    "# save for use in production environment\n",
    "torch.jit.save(script, \"model_done.pt\")\n",
    "# model2.eval()\n",
    "# scripted_searcher = torch.jit.script(model2(input_tensor,target_tensor,seq_lengths))\n",
    "# scripted_searcher.save(\"scripted.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Normal PyTorch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluateRandomly(encoder, decoder, n=10):\n",
    "    for i in range(n):\n",
    "        pair = random.choice(pairs)\n",
    "        print('>', pair[0])\n",
    "        print('=', pair[1])\n",
    "        output_words, attentions = evaluate(encoder, decoder, pair[0])\n",
    "        output_sentence = ' '.join(output_words)\n",
    "        print('<', output_sentence)\n",
    "        print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder1.load_state_dict(torch.load('enc_saved1'))\n",
    "encoder1.to(device)\n",
    "attn_decoder1.load_state_dict(torch.load('dec_saved1'))\n",
    "attn_decoder1.to(device)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 237.52300000000002,
   "position": {
    "height": "259.523px",
    "left": "1042.36px",
    "right": "20px",
    "top": "121px",
    "width": "336px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "block",
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
